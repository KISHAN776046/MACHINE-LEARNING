{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85afa4a-af98-4a09-a61c-e47ab2669d38",
   "metadata": {},
   "source": [
    "1.What is Simple Linear Regression?\n",
    "\n",
    "Simple Linear Regression is a statistical method to model the relationship between two variables by fitting a straight line. It predicts the value of a dependent variable (target) based on one independent variable (feature).\n",
    "\n",
    "The equation is:  \n",
    "y = b0 + b1 * x  \n",
    "where,  \n",
    "- y = predicted value  \n",
    "- b0 = intercept  \n",
    "- b1 = slope (coefficient)  \n",
    "- x = independent variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b94470-442d-4fcd-a71f-256d875d04f9",
   "metadata": {},
   "source": [
    "2.What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "1. Linearity:The relationship between the independent and dependent variable is linear.\n",
    "2. Independence: Observations are independent of each other.\n",
    "3. Homoscedasticity:Constant variance of errors (residuals) across all levels of the independent variable.\n",
    "4. Normality:The residuals (errors) are normally distributed.\n",
    "5. No multicollinearity: (In simple linear regression, only one predictor, so this mainly applies to multiple regression.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d5125-39d3-4587-8b51-307882d45611",
   "metadata": {},
   "source": [
    "3.What does the coefficient m represent in the equation Y = mX + c?\n",
    "\n",
    "The coefficient m represents the slope of the line. It indicates the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bc827-d123-44c5-9e1a-dc25dfb3fd18",
   "metadata": {},
   "source": [
    "4.What does the intercept c represent in the equation Y = mX + c?\n",
    "\n",
    "The intercept c represents the value of Y when X is zero. It is the point where the regression line crosses the Y-axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03a530-d38a-4849-98eb-2cd3a8849573",
   "metadata": {},
   "source": [
    "5.How do we calculate the slope m in Simple Linear Regression?\n",
    "\n",
    "Formula for slope (m):\n",
    "m = covariance(X, Y) / variance(X)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Example data\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "Y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "Calculate means\n",
    "mean_x = np.mean(X)\n",
    "mean_y = np.mean(Y)\n",
    "\n",
    " Calculate numerator and denominator for slope\n",
    "numerator = np.sum((X - mean_x) * (Y - mean_y))\n",
    "denominator = np.sum((X - mean_x) ** 2)\n",
    "\n",
    "Calculate slope\n",
    "m = numerator / denominator\n",
    "\n",
    "print(\"Slope (m):\", m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28b6ea-e8ac-4785-acd0-84e61fe950be",
   "metadata": {},
   "source": [
    "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\n",
    "The least squares method is used to find the best-fitting line by minimizing the sum of the squared differences (errors) between the observed values and the predicted values. It helps to find the slope (m) and intercept (c) that minimize these errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b963d3-0d33-487b-a1f2-c0384d5ad53b",
   "metadata": {},
   "source": [
    "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "\n",
    "R² measures how well the regression line fits the data. It represents the proportion of the variance in the dependent variable explained by the independent variable.\n",
    "\n",
    "- R² = 1 means perfect fit.\n",
    "- R² = 0 means the model explains none of the variance.\n",
    "- Higher R² indicates better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2416-a319-4138-bdc6-d949fa17f62b",
   "metadata": {},
   "source": [
    "8. What is Multiple Linear Regression?\n",
    "\n",
    "Multiple Linear Regression is a statistical method used to model the relationship between one dependent variable and two or more independent variables by fitting a linear equation.\n",
    "\n",
    "The equation is:  \n",
    "y = b0 + b1*x1 + b2*x2 + ... + bn*xn  \n",
    "\n",
    "where,  \n",
    "- y = predicted value  \n",
    "- b0 = intercept  \n",
    "- b1, b2, ..., bn = coefficients for each independent variable x1, x2, ..., xn  \n",
    "- n = number of independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f857ac91-f01b-4c55-9090-37954b68e7e4",
   "metadata": {},
   "source": [
    "9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "\n",
    "- Simple Linear Regression uses one independent variable to predict the dependent variable.\n",
    "- Multiple Linear Regression uses two or more independent variables to predict the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a84317-8280-4a7a-969a-a75fb85d99ec",
   "metadata": {},
   "source": [
    "10.What are the key assumptions of Multiple Linear Regression?\n",
    "\n",
    "Linearity: The relationship between dependent and independent variables is linear.\n",
    "Independence: Observations are independent.\n",
    "Homoscedasticity:Constant variance of residuals across all levels of independent variables.\n",
    "Normality: Residuals are normally distributed.\n",
    "No multicollinearity: Independent variables are not highly correlated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99a4c9-dc87-40c8-a96f-e428e57de0e9",
   "metadata": {},
   "source": [
    "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "\n",
    "Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
    "\n",
    "Effects:\n",
    "- It violates the assumption of homoscedasticity.\n",
    "- Causes inefficient estimates and unreliable hypothesis tests.\n",
    "- Leads to biased standard errors, affecting confidence intervals and p-values.\n",
    "- Makes model predictions less reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83a7d0-cacc-48d1-b71c-a9ace0c2381d",
   "metadata": {},
   "source": [
    "12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "\n",
    "- Remove or combine highly correlated features.\n",
    "- Use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
    "- Apply regularization methods such as Ridge or Lasso regression.\n",
    "- Collect more data if possible.\n",
    "- Check and carefully select independent variables based on domain knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f9214-11dc-4044-838b-bb693e4ec2a0",
   "metadata": {},
   "source": [
    "13.What are some common techniques for transforming categorical variables for use in regression models?\n",
    "\n",
    "- One-Hot Encoding: Converts each category into binary columns (0 or 1).\n",
    "- Label Encoding: Assigns each category a unique integer (useful for ordinal categories).\n",
    "- Ordinal Encoding: Assigns ordered integers to categories with a meaningful order.\n",
    "- Binary Encoding:Converts categories into binary digits to reduce dimensionality.\n",
    "- Target Encoding: Replaces categories with the mean of the target variable for that category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f0ec1-f781-408d-8aca-8f0b0f0a13cc",
   "metadata": {},
   "source": [
    "14.What is the role of interaction terms in Multiple Linear Regression?\n",
    "\n",
    "Interaction terms capture the combined effect of two or more independent variables on the dependent variable, showing how the effect of one variable changes depending on the level of another.\n",
    "\n",
    "They help model complex relationships that are not purely additive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319d2ea-2540-4e0a-b105-29ad408d5fbb",
   "metadata": {},
   "source": [
    "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "\n",
    "- In Simple Linear Regression, the intercept represents the predicted value of the dependent variable when the single independent variable is zero.\n",
    "- In Multiple Linear Regression, the intercept represents the predicted value when all independent variables are zero simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e1813-3e94-4f4a-bb46-6a5db7f66c30",
   "metadata": {},
   "source": [
    "16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "\n",
    "The slope indicates the amount of change in the dependent variable for a one-unit increase in the independent variable. It shows the direction and strength of the relationship.\n",
    "\n",
    "A positive slope means the dependent variable increases as the independent variable increases, while a negative slope means it decreases.\n",
    "\n",
    "The slope directly affects predictions by determining how much the output changes with input changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53652428-40a1-42a3-bd8e-293d2d336089",
   "metadata": {},
   "source": [
    "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "\n",
    "The intercept represents the expected value of the dependent variable when all independent variables are zero. It provides a baseline or starting point for the model's predictions, helping to understand where the regression line or plane crosses the dependent variable axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805f212-2fd5-44ef-9e09-8ceb91492569",
   "metadata": {},
   "source": [
    "18.What are the limitations of using R² as a sole measure of model performance?\n",
    "\n",
    "- R² does not indicate if the model is appropriate or unbiased.\n",
    "- It can be artificially high with many predictors (overfitting).\n",
    "- Does not measure predictive accuracy on new data.\n",
    "- Cannot detect non-linear relationships.\n",
    "- Does not show if variables are significant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7cfba2-53ed-44d7-9070-42d043daf618",
   "metadata": {},
   "source": [
    "19. How would you interpret a large standard error for a regression coefficient?\n",
    "\n",
    "A large standard error means the estimate of the coefficient is less precise and has more variability. It indicates uncertainty in the coefficient’s value, suggesting the predictor may not be reliably associated with the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4384815-8b0d-4ee8-ab9a-e6c8cf5632a5",
   "metadata": {},
   "source": [
    "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\n",
    "- Identification:\n",
    "  In residual plots, heteroscedasticity appears as a pattern where the spread (variance) of residuals changes across levels of predicted values (e.g., funnel shape or increasing/decreasing spread).\n",
    "\n",
    "- Importance: \n",
    "  Addressing heteroscedasticity is important because it violates regression assumptions, leading to inefficient estimates, biased standard errors, and unreliable hypothesis tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9c08e-9593-4b2e-9264-78717d052345",
   "metadata": {},
   "source": [
    "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "\n",
    "It indicates that while the model explains a lot of variance (high R²), it may include unnecessary or irrelevant predictors. The adjusted R² penalizes adding useless variables, so a low adjusted R² suggests overfitting or that some predictors don't improve the model significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388af13-42d8-45c1-8635-34e4f84a801b",
   "metadata": {},
   "source": [
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "\n",
    "Scaling variables ensures all features contribute equally by putting them on the same scale. It improves model training stability and convergence, especially when using regularization techniques like Ridge or Lasso regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1233f9-ccb3-4f07-9e49-c8e066ab58d1",
   "metadata": {},
   "source": [
    "23. What is polynomial regression?\n",
    "\n",
    "Polynomial regression is an extension of linear regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
    "\n",
    "The equation:  \n",
    "y = b0 + b1*x + b2*x² + ... + bn*xⁿ  \n",
    "\n",
    "It allows modeling of non-linear relationships by fitting a curve instead of a straight line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862d506-a95a-4300-8443-c7b170dccbc5",
   "metadata": {},
   "source": [
    "24. How does polynomial regression differ from linear regression?\n",
    "\n",
    "- **Linear regression** models a straight-line relationship between independent and dependent variables.\n",
    "- **Polynomial regression** models a curved relationship by including higher-degree terms (e.g., x², x³), capturing non-linear patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa55d6-6263-47f5-9555-15148de7d319",
   "metadata": {},
   "source": [
    "25. When is polynomial regression used?\n",
    "\n",
    "Polynomial regression is used when the relationship between the independent and dependent variables is **non-linear** but can be approximated by a polynomial curve. It helps model curved trends that linear regression cannot capture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0ecd7-da34-4019-871b-c63f211813f3",
   "metadata": {},
   "source": [
    "26. What is the general equation for polynomial regression?\n",
    "\n",
    "y = b0 + b1*x + b2*x² + b3*x³ + ... + bn*xⁿ\n",
    "\n",
    "where,  \n",
    "- y = predicted value  \n",
    "- b0 = intercept  \n",
    "- b1, b2, ..., bn = coefficients  \n",
    "- x, x², ..., xⁿ = independent variable raised to powers from 1 to n  \n",
    "- n = degree of the polynomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e75e6-5859-4992-98ff-8a3aa3bdc53b",
   "metadata": {},
   "source": [
    "27. Can polynomial regression be applied to multiple variables?\n",
    "\n",
    "Yes. Polynomial regression can be extended to multiple variables by including polynomial terms and interaction terms of the independent variables, allowing modeling of non-linear relationships in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e296d-5bfd-4c44-8b67-89db7b66d377",
   "metadata": {},
   "source": [
    "28. What are the limitations of polynomial regression?\n",
    "\n",
    "- Prone to **overfitting**, especially with high-degree polynomials.\n",
    "- Can produce **oscillations** and unstable predictions outside the training data range.\n",
    "- **Difficult to interpret** coefficients as the degree increases.\n",
    "- Sensitive to **outliers**.\n",
    "- Requires careful selection of polynomial degree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25287fc5-0c27-4d2e-9224-b081dd04813c",
   "metadata": {},
   "source": [
    "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\n",
    "- **Cross-validation:** Assess model performance on unseen data.\n",
    "- **Adjusted R²:** Penalizes adding unnecessary polynomial terms.\n",
    "- **Mean Squared Error (MSE):** Lower values indicate better fit.\n",
    "- **Root Mean Squared Error (RMSE):** Provides error in original units.\n",
    "- **Visual inspection:** Plotting fitted curve against data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec825efa-0ac3-4bd4-a71a-67dfa5861b54",
   "metadata": {},
   "source": [
    "30. Why is visualization important in polynomial regression?\n",
    "\n",
    "Visualization helps to:\n",
    "- Understand the shape of the relationship between variables.\n",
    "- Detect underfitting or overfitting by comparing the curve to data points.\n",
    "- Communicate model behavior clearly.\n",
    "- Identify outliers and patterns that affect the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce2d10-856b-4b6e-8125-11b585d27fc0",
   "metadata": {},
   "source": [
    "31.How is polynomial regression implemented in Python?\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 6, 14, 28, 45])\n",
    "\n",
    "Transform features to polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    " Fit Linear Regression on transformed features\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "Predict\n",
    "y_pred = model.predict(X_poly)\n",
    "print(\"Predicted values:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1ae79-5ef3-4458-ab4a-e8f955d8cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
